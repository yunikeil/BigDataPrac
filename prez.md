

# 1 СЛАЙД

Airflow — открытая платформа управления данными, создаёт, планирует и мониторит цепочки задач. Использует направленный ациклический граф (DAG) для представления рабочего процесса. Предоставляет интерфейс командной строки и веб-интерфейс, гибок и расширяем. Поддерживает различные источники данных и инструменты выполнения задач, такие как Spark, Hadoop, SQL-серверы и другие


# АРХИТЕКТУРА


# Некст слайд

Airflow состоит из нескольких компонентов, которые работают вместе для обеспечения функциональности и возможностей рабочего процесса.

Database
Используется для хранения информации о DAG, задачах, статусе выполнения, расписании и других метаданных. Airflow поддерживает различные системы управления базами данных, такие как PostgreSQL, MySQL, SQLite и другие.

Worker в Airflow
Процесс, который фактически выполняет задачи, определённые в рабочем процессе данных.

Scheduler (Планировщик)
Отвечает за планирование выполнения задач на основе определённого расписания. Он отслеживает исполнение DAG и запускает задачи в соответствии с их зависимостями. Также он сканирует и сериализует DAG и записывает их в БД. 

Executor (Исполнитель)
Отвечает за запуск фактического выполнения задач в Airflow.

Web server (Веб-сервер)
Предоставляет веб-интерфейс для управления DAG, выполняемых задач и мониторинга выполнения рабочего процесса. Здесь пользователи могут просматривать и запускать задачи, анализировать журналы выполнения и многое другое.


Варианты исполнителей:
1 LocalExecutor: наиболее распространённый и простой в использовании исполнитель.
2 CeleryExecutor: использует инфраструктуру Celery для распределённого выполнения задач. (Я использую данный тип)
3 KubernetesExecutor: использует Kubernetes в качестве платформы для выполнения задач. Каждая задача запускается в виде отдельного пода Kubernetes, что обеспечивает изоляцию и масштабируемость.
4 DaskExecutor: использует фреймворк Dask для выполнения задач. DaskExecutor позволяет выполнять задачи параллельно с использованием возможностей распределённых вычислений Dask.



# ОСНОВНЫЕ АБСТРАКЦИИ


# Некст слайд

DAG (Directed Acyclic Graph) — набор задач (Task), связанных друг с другом и организованных в виде направленного ациклического графа. Это понятие используется в Airflow для описания и управления рабочим процессом и его зависимостями.

DAG в Airflow определяет порядок выполнения задач и их зависимости друг от друга. Задачи могут быть запланированы для выполнения в определённые моменты времени и выполняться автоматически в соответствии с заданным графиком.

DAG включает в себя следующие компоненты:

 - Имя: уникальное имя, идентифицирующее конкретный DAG;
 - Задачи (Tasks): операции или работы, которые нужно выполнить в рамках DAG;
 - Зависимости между задачами: определяются с помощью операторов и позволяют контролировать порядок выполнения задач;
 - Параметры и настройки выполнения: определяют время запуска, интервал выполнения, аргументы задач и другие настройки.


# Некст слайд

DagRun в Airflow — конкретное выполнение DAG в определённый момент времени. Когда DAG запускается, создаётся новый экземпляр DagRun, который представляет собой одну конкретную итерацию или запуск DAG. DagRun обеспечивает связь между DAG и его выполнениями.

DagRun содержит метаданные о запуске: дата и время начала, дата и время окончания, статус выполнения. Он также может содержать другую полезную информацию, которая может быть полезна для мониторинга и управления выполнением DAG.


# Некст слайд

Оператор (Operator) в Airflow — это класс, предназначенный для выполнения конкретной задачи внутри DAG. Оператор определяет поведение и логику задачи, которую необходимо решить, и предоставляет методы для её выполнения.


# Некст слайд

Все операторы (PythonOperator, SqlOperator, BashOperator и прочие) наследуются от общего родительского оператора – BaseOperator.

Также можно написать свой оператор. Для этого необходимо перегрузить метод execute класса BaseOperator.



# Некст слайд

Task в Airflow — фундаментальная единица работы или операция, которую необходимо выполнить в рамках DAG

Каждая задача в DAG имеет свои собственные параметры и свойства: название, оператор, зависимости и аргументы. Задачи в DAG могут быть связаны друг с другом через их зависимости, образуя граф выполнения.

Очень грубо говоря Задача - Объект класса оператора.


TaskInstance в Airflow — это конкретный экземпляр задачи (Task) в рамках выполнения DAG. Каждая задача в DAG имеет свою TaskInstance, которая представляет её выполнение в определённый момент времени. 

TaskInstance хранит информацию о состоянии задачи: статус выполнения, время начала и завершения, параметры выполнения и журнал выполнения. Он также отслеживает зависимости между задачами и управляет порядком выполнения задач в рамках DAG.

TaskInstance генерируется планировщиком Airflow для каждой задачи, когда она готова к выполнению. Он предоставляет интерфейс для мониторинга выполнения задачи, отображения журналов выполнения, установки параметров выполнения и управления процессом выполнения задачи.



# Некст слайд

TaskGroup в Airflow — конструкция для группировки задач внутри DAG. TaskGroup позволяет создавать вложенную структуру, где задачи могут быть организованы в подзадачи или группы задач. Это удобно для логической организации и визуализации сложных DAG, состоящих из множества связанных задач.


# Некст слайд

Sensor в Airflow — это особый тип оператора, который используется для ожидания определённых условий перед выполнением следующей задачи в DAG. Он позволяет задержать выполнение следующей задачи, пока не будут выполнены определённые условия или события.

Sensor действует как «датчик», который наблюдает или проверяет состояние или событие, и когда это состояние или событие становится истинным или происходит, Sensor передаёт управление следующей задаче.

Все сенсоры так же имеют единый родительский класс, как и операторы –   BaseSensorOperator. Также можно написать свой сенсор. Для этого необходимо перегрузить метод poke класса BaseSensorOperator. При этом метод poke должен возвращать True или False!


# Некст слайд (10)

BranchPythonOperator — используется для определения условия ветвления в DAG. Он позволяет принимать решение, какая задача должна быть выполнена на основе результата выполнения предыдущей задачи или других условий.

BranchPythonOperator принимает в качестве аргумента пользовательский Python-скрипт или функцию, которая определяет условие и возвращает идентификатор следующей задачи, которая должна быть выполнена. Идентификатор задачи представляется как строка и соответствует имени задачи в DAG.

Также можно написать свой branch-оператор. Для этого необходимо перегрузить метод choose_branch класса BaseBranchOperator.



# Некст слайд

Variables в Airflow — способ хранения и управления конфигурационными переменными в рамках Airflow. Они позволяют сохранять и извлекать значения переменных, которые могут быть использованы в DAG или операторах.

Variables можно использовать для хранения широкого спектра данных: строки, числа, списки, словари. Также можно хранить параметры, настройки или любые другие значения, необходимые для выполнения задачи или DAG.

Пример получения variables (переменных) при помощи макроса 


#  Некст слайд

 
Connection в Airflow — настройки подключения к внешним ресурсам, таким как базы данных, API-сервисы, файловые хранилища и другие. Он используется для хранения информации о параметрах подключения: хост, порт, имя пользователя, пароль и другие аутентификационные данные, необходимые для доступа к внешним системам. 

Airflow позволяет определить и управлять Connection через веб-интерфейс или API. Один раз определённая Connection может быть использована в разных DAG и операторах. Подключение может быть использовано для обмена данными, выполнения запросов, загрузки и сохранения файлов и других операций, требующих доступа к внешним ресурсам. 



# Некст слайд

ds - DATA_INTERVAL_START/EXEC_DATE представляет текущую дату выполнения задачи в формате YYYY-MM-DD.

Jinja в Airflow — шаблонизатор, который используется для динамической генерации кода и данных во время выполнения DAG. Jinja обеспечивает возможность вставлять переменные, условные операторы, циклы и другие конструкции в шаблоны, что позволяет создавать динамические и гибкие пайплайны данных.

Airflow использует Jinja для поддержки переменных, передачи параметров и выполнения операций в контексте выполнения задачи. Он интегрируется с операторами и шаблонными файлами Airflow, позволяя вам создавать динамические настройки и генерировать код и данные на основе переменных и условий.



# ТЕПЕРЬ ПРО ПЛАНИРОВАНИЕ



# Некст слайд

Запуск происходит в конце интервала, при этом execution_date равен началу интервала.

start_date - Это дата, с которой начинается расписание DAG. Она должна быть статической и указывать на начало интервала данных. Например, если ты хочешь запускать DAG-граф ежедневно, то start_date должна быть датой, а не текущим временем. Start_date не означает, что DAG запустится в эту дату, а только то, что он не запустится до неё.

execution_date (logical) - Это дата, которая присваивается каждому запуску DAG. Она указывает на начало интервала данных, для которого выполняются задачи. Execution_date всегда соответствует start_date или одному из его повторений по расписанию.

schedule_interval - Это атрибут, который определяет интервал времени между последовательными запусками DAG.


Пример: 
Если start_date равен 2021-01-01, а schedule_interval равен @daily, то первый запуск DAG будет иметь execution_date равный 2021-01-01, а второй — 2021-01-02.

Важно понимать, что задачи в Airflow запускаются не в момент execution_date, а в конце интервала данных. Это означает, что первый запуск DAG с start_date 2021-01-01 и schedule_interval @daily произойдёт не в 2021-01-01, а в 2021-01-02. Это сделано для того, чтобы убедиться, что все данные за интервал доступны для обработки.



# Некст слайд (15)

Catchup — процесс планирования и выполнения всех предыдущих запусков DAG, которые были бы запланированы, если бы DAG был создан и запущен ранее.


#  Некст слайд

Бывают случаи, когда нам необходимо запустить задачи за даты, предшествующие start_date. Для этого нужно использовать backfill.

Backfill в Airflow — это утилита командной строки.


# Некст слайд

Scheduler обращается в базу данных (PostgreSQL) и формирует список задач к запуску. Это будет видно в веб-сервере, который позволяет отслеживать весь процесс.

Scheduler помещает данный список в брокер сообщений (в нашем случае Redis).

Worker берёт задачу из очереди.

Worker выполняет задачу и записывает значение в базу данных.

Scheduler помечает, что задача была выполнена.


# Некст слайд

Иногда бывают ситуации, когда количество задач может меняться от запуска к запуску DAG. Для таких случаев существует DynamicTask.

DynamicTask — это способ создания задач на лету, в зависимости от текущих данных, а не заранее заданного количества задач. 

Для этого используется функция expand(), которая принимает список или словарь параметров и создаёт по одной копии задачи для каждого входа. Также можно обрабатывать собранный вывод сопоставленных задач, например, с помощью функции sum(). 


# Некст слайд


Помимо сенсоров и TriggerDagRunOperator, в Airflow мы также можем использовать Dataset — это альтернативный способ планирования, где мы завязываемся не на времени выполнения (cron), а на изменении данных.

Для использования Dataset в своих DAG можно указать его в параметрах outlets и schedule своих операторов и DAG. Параметр outlets определяет, какие Dataset обновляются задачей-производителем, а параметр schedule определяет, какие Dataset должны быть обновлены, чтобы запустить DAG-потребитель. 























